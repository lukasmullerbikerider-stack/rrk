import os
import time
import json
import logging
import streamlit as st
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime
from jdatetime import datetime as jdatetime
import subprocess
import sys
import tempfile

# ----------------------------------
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ
# ----------------------------------
st.set_page_config(page_title="RRK Company Extractor", page_icon="ğŸ¢", layout="wide")

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Create a temporary directory for each run
user_data_dir = tempfile.mkdtemp()

# Setup Chrome options
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--incognito")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--window-size=1400,1000")
chrome_options.add_argument("--disable-notifications")

# ----------------------------------
# ØªÙˆØ§Ø¨Ø¹ Selenium
# ----------------------------------
def scrape_company_ads(query):
    """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ rrk.ir"""
    driver, wait = setup_driver()
    ad_data = []

    try:
        driver.get("https://www.rrk.ir/")
        search_box = wait.until(EC.presence_of_element_located((By.ID, "P0_SEARCH_ITEM")))
        search_box.clear()
        search_box.send_keys(query)
        driver.find_element(By.ID, "BTN_ADVANCEDSEARCH").click()
        time.sleep(3)

        # ÙˆØ±ÙˆØ¯ Ø¨Ù‡ Ø¨Ø®Ø´ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§
        wait.until(EC.element_to_be_clickable((By.CLASS_NAME, "t-LinksList-link"))).click()
        time.sleep(5)

        current_page = 1
        while True:
            ad_links = get_links(driver)
            if not ad_links:
                break

            for tag in ad_links:
                href = tag.get("href")
                if not href.startswith("/ords/r/rrs/rrs-front/f-detail-ad"):
                    continue
                url = "https://rrk.ir" + href

                driver.execute_script("window.open('');")
                driver.switch_to.window(driver.window_handles[1])
                driver.get(url)
                soup = BeautifulSoup(driver.page_source, "html.parser")

                try:
                    data = extract_fields(driver, soup)
                    data["url"] = url
                    ad_data.append(data)
                except Exception as e:
                    logging.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ú¯Ù‡ÛŒ: {e}")
                finally:
                    driver.close()
                    driver.switch_to.window(driver.window_handles[0])
                    time.sleep(2)

            # ØµÙØ­Ù‡ Ø¨Ø¹Ø¯
            next_buttons = driver.find_elements(By.CSS_SELECTOR, "ul.a-GV-pageSelector-list li button.a-GV-pageButton")
            next_btn = next((b for b in next_buttons if b.text.isdigit() and int(b.text) == current_page + 1), None)
            if not next_btn:
                break
            driver.execute_script("arguments[0].click();", next_btn)
            current_page += 1
            time.sleep(5)

    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø§: {e}")
    finally:
        driver.quit()

    return ad_data

def setup_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--user-data-dir=/tmp/unique_profile_" + str(time.time()))
    # Merge with global chrome_options for better compatibility
    for arg in chrome_options.arguments:
        if arg not in options.arguments:
            options.add_argument(arg.split('=')[0] if '=' in arg else arg)
    driver = webdriver.Chrome(options=options)

    driver.implicitly_wait(10)
    wait = WebDriverWait(driver, 60)
    return driver, wait

def get_links(driver):
    soup = BeautifulSoup(driver.page_source, "html.parser")
    return soup.select("a[href*='/ords/r/rrs/rrs-front/f-detail-ad']")

def extract_fields(driver, soup):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¢Ú¯Ù‡ÛŒ Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª"""
    fields = {
        "Ø´Ù…Ø§Ø±Ù‡ Ù¾ÛŒÚ¯ÛŒØ±ÛŒ": driver.find_element(By.ID, "P28_REFERENCENUMBER").get_attribute("value"),
        "Ø´Ù…Ø§Ø±Ù‡ Ù†Ø§Ù…Ù‡": driver.find_element(By.ID, "P28_INDIKATORNUMBER").get_attribute("value"),
        "ØªØ§Ø±ÛŒØ® Ù†Ø§Ù…Ù‡": driver.find_element(By.ID, "P28_SABTDATE").get_attribute("value"),
        "Ù†Ø§Ù… Ø´Ø±Ú©Øª": driver.find_element(By.ID, "P28_COMPANYNAME").get_attribute("value"),
        "Ø´Ù†Ø§Ø³Ù‡ Ù…Ù„ÛŒ Ø´Ø±Ú©Øª": driver.find_element(By.ID, "P28_SABTNATIONALID").get_attribute("value"),
        "Ø´Ù…Ø§Ø±Ù‡ Ø«Ø¨Øª": driver.find_element(By.ID, "P28_SABTNUMBER").get_attribute("value"),
        "Ø´Ù…Ø§Ø±Ù‡ Ø±ÙˆØ²Ù†Ø§Ù…Ù‡": driver.find_element(By.ID, "P28_NEWSPAPERNO").get_attribute("value"),
        "ØªØ§Ø±ÛŒØ® Ø±ÙˆØ²Ù†Ø§Ù…Ù‡": driver.find_element(By.ID, "P28_NEWSPAPERDATE").get_attribute("value"),
        "Ø´Ù…Ø§Ø±Ù‡ ØµÙØ­Ù‡ Ø±ÙˆØ²Ù†Ø§Ù…Ù‡": driver.find_element(By.ID, "P28_PAGENUMBER").get_attribute("value"),
        "ØªØ¹Ø¯Ø§Ø¯ Ù†ÙˆØ¨Øª Ø§Ù†ØªØ´Ø§Ø±": driver.find_element(By.ID, "P28_HCNEWSSTAGE").get_attribute("value")
    }
    dynamic = soup.select_one("a-dynamic-content")
    fields["Ù…ØªÙ† Ø¢Ú¯Ù‡ÛŒ"] = dynamic.get_text(" ", strip=True) if dynamic else soup.get_text(" ", strip=True)
    return fields

# 2ï¸âƒ£ --- ØªÙ†Ø¸ÛŒÙ… API Key ---
apikey = "AIzaSyAALSr7TI81SZ6e0X9tLk14GJJk37CkMgQ"
# ------------------ ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ ------------------

def llm(data):
    """Ø§Ø±Ø³Ø§Ù„ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ù…Ø¯Ù„ Gemini Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ø®Ø±ÙˆØ¬ÛŒ JSON"""
    try:
        prompt = json.dumps(data, ensure_ascii=False, indent=2)
        system_instruction = """
        Ù†Ù‚Ø´: ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ø­Ù‚ÙˆÙ‚ÛŒ Ùˆ Ø´Ø±Ú©ØªÛŒ Ù…ØªØ®ØµØµ Ø¯Ø± Ø±ÙˆØ²Ù†Ø§Ù…Ù‡ Ø±Ø³Ù…ÛŒ.
        ÙˆØ¸ÛŒÙÙ‡: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª Ø§Ø² Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ ØªØ§Ø±ÛŒØ®ÛŒ.
        Ø®Ø±ÙˆØ¬ÛŒ JSON Ø·Ø¨Ù‚ Ù‚Ø§Ù„Ø¨ Ù…Ø´Ø®Øµ.
        """

        model = genai.GenerativeModel(
            model_name="gemini-2.5-pro",
            system_instruction=system_instruction
        )

        response = model.generate_content(
            prompt,
            generation_config={
                "response_mime_type": "application/json",
                "temperature": 0.2
            }
        )

        result = json.loads(response.text)
        return result

    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø¯Ù„: {e}")
        return None


def shamsi_to_miladi(date_str):
    if not date_str or date_str == 'null':
        return datetime.now()
    try:
        year, month, day = map(int, date_str.split('/'))
        return jdatetime.date(year, month, day).togregorian()
    except Exception:
        return datetime.now()


def charts(data):
    """Ù†Ù…Ø§ÛŒØ´ Ú†Ø§Ø±Øª Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª Ø¯Ø± Streamlit"""
    if not data or 'Ø§Ø¹Ø¶Ø§ÛŒ Ø³Ø§Ø¨Ù‚ Ø´Ø±Ú©Øª' not in data:
        st.error("âŒ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª.")
        return

    df = pd.DataFrame(data['Ø§Ø¹Ø¶Ø§ÛŒ Ø³Ø§Ø¨Ù‚ Ø´Ø±Ú©Øª'])
    df['ØªØ§Ø±ÛŒØ®_Ø´Ø±ÙˆØ¹_Ù…ÛŒÙ„Ø§Ø¯ÛŒ'] = df['ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹'].apply(shamsi_to_miladi)
    df['ØªØ§Ø±ÛŒØ®_Ù¾Ø§ÛŒØ§Ù†_Ù…ÛŒÙ„Ø§Ø¯ÛŒ'] = df['ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù†'].apply(shamsi_to_miladi)

    def categorize_position(position):
        if not isinstance(position, str): return 'Ø³Ø§ÛŒØ±'
        if 'Ù…Ø¯ÛŒØ±Ø¹Ø§Ù…Ù„' in position:
            return 'Ù…Ø¯ÛŒØ±Ø¹Ø§Ù…Ù„'
        elif any(x in position for x in ['Ø±Ø¦ÛŒØ³', 'Ù†Ø§ÛŒØ¨', 'Ø¹Ø¶Ùˆ Ù‡ÛŒØ¦Øª', 'Ø¹Ø¶Ùˆ Ù‡ÛŒØ§Øª']):
            return 'Ù‡ÛŒØ¦Øª Ù…Ø¯ÛŒØ±Ù‡'
        elif 'Ø¨Ø§Ø²Ø±Ø³' in position:
            return 'Ø¨Ø§Ø²Ø±Ø³'
        else:
            return 'Ø³Ø§ÛŒØ±'

    df['Ø¯Ø³ØªÙ‡'] = df['Ø³Ù…Øª'].apply(categorize_position)
    df = df.sort_values('ØªØ§Ø±ÛŒØ®_Ø´Ø±ÙˆØ¹_Ù…ÛŒÙ„Ø§Ø¯ÛŒ')

    color_map = {
        'Ù…Ø¯ÛŒØ±Ø¹Ø§Ù…Ù„': '#ff7f0e',
        'Ù‡ÛŒØ¦Øª Ù…Ø¯ÛŒØ±Ù‡': '#1f77b4',
        'Ø¨Ø§Ø²Ø±Ø³': '#8c564b',
        'Ø³Ø§ÛŒØ±': '#7f7f7f'
    }

    fig = make_subplots(
        rows=3, cols=1,
        subplot_titles=('ØªØ§ÛŒÙ…â€ŒÙ„Ø§ÛŒÙ† Ú©Ø§Ù…Ù„ Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª', 'ØªØ§ÛŒÙ…â€ŒÙ„Ø§ÛŒÙ† Ù…Ø¯ÛŒØ±Ø¹Ø§Ù…Ù„ Ùˆ Ø±Ø¦ÛŒØ³ Ù‡ÛŒØ¦Øª', 'Ø¢Ù…Ø§Ø± Ø³Ù…Øªâ€ŒÙ‡Ø§'),
        specs=[[{"type": "scatter"}], [{"type": "scatter"}], [{"type": "bar"}]],
        vertical_spacing=0.12
    )

    for _, row in df.iterrows():
        color = color_map.get(row['Ø¯Ø³ØªÙ‡'], '#7f7f7f')
        fig.add_trace(
            go.Scatter(
                x=[row['ØªØ§Ø±ÛŒØ®_Ø´Ø±ÙˆØ¹_Ù…ÛŒÙ„Ø§Ø¯ÛŒ'], row['ØªØ§Ø±ÛŒØ®_Ù¾Ø§ÛŒØ§Ù†_Ù…ÛŒÙ„Ø§Ø¯ÛŒ']],
                y=[row['Ù†Ø§Ù…'], row['Ù†Ø§Ù…']],
                mode='lines+markers',
                name=row['Ø³Ù…Øª'],
                line=dict(color=color, width=5),
                marker=dict(size=7, color=color),
                hovertemplate=f"{row['Ù†Ø§Ù…']}<br>{row['Ø³Ù…Øª']}<br>{row['ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹']} ØªØ§ {row['ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù†']}"
            ),
            row=1, col=1
        )

    filtered = df[df['Ø¯Ø³ØªÙ‡'].isin(['Ù…Ø¯ÛŒØ±Ø¹Ø§Ù…Ù„', 'Ù‡ÛŒØ¦Øª Ù…Ø¯ÛŒØ±Ù‡'])]
    for _, row in filtered.iterrows():
        color = color_map.get(row['Ø¯Ø³ØªÙ‡'], '#7f7f7f')
        fig.add_trace(
            go.Scatter(
                x=[row['ØªØ§Ø±ÛŒØ®_Ø´Ø±ÙˆØ¹_Ù…ÛŒÙ„Ø§Ø¯ÛŒ'], row['ØªØ§Ø±ÛŒØ®_Ù¾Ø§ÛŒØ§Ù†_Ù…ÛŒÙ„Ø§Ø¯ÛŒ']],
                y=[f"{row['Ù†Ø§Ù…']} ({row['Ø³Ù…Øª']})"] * 2,
                mode='lines+markers',
                line=dict(color=color, width=6)
            ),
            row=2, col=1
        )

    counts = df['Ø¯Ø³ØªÙ‡'].value_counts()
    fig.add_trace(
        go.Bar(x=counts.index, y=counts.values, text=counts.values, textposition='auto'),
        row=3, col=1
    )

    fig.update_layout(height=1200, title="ğŸ“Š Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª", template='plotly_white')
    st.plotly_chart(fig, use_container_width=True)


# ------------------ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Streamlit ------------------
st.set_page_config(page_title="RRK Analyzer", layout="wide")
st.title("ğŸ¢ RRK.ir â€“ Company Ads Extractor & Analyzer")

tab1, tab2, tab3 = st.tabs(["ğŸ•µï¸ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø±Ú©Øª", "ğŸ“‚ Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª", "ğŸ“ˆ ØªØ§ÛŒÙ…â€ŒÙ„Ø§ÛŒÙ† Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª"])

# --- ØªØ¨ 1 ---
with tab1:
    st.info("Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø§ ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ù†Ø§Ù… Ø´Ø±Ú©ØªØŒ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ù†ÛŒØ¯ (Ù†Ù…ÙˆÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ).")
    query = st.text_input("ğŸ” Ù†Ø§Ù… Ø´Ø±Ú©Øª ÛŒØ§ Ø´Ù†Ø§Ø³Ù‡ Ù…Ù„ÛŒ:")
    if st.button("Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬"):
        st.warning("Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù‡Ù†ÙˆØ² Ø¨Ù‡ ØªØ§Ø¨Ø¹ scrape_company_ads Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯ (Ø¯Ø± Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡ ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª).")

# --- ØªØ¨ 2 ---
with tab2:
    st.markdown("### ğŸ“ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ JSON Ø§Ø² Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§")
    uploaded = st.file_uploader("ÙØ§ÛŒÙ„ JSON Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯", type=["json"])
    if uploaded:
        ads = json.load(uploaded)
        st.success(f"{len(ads)} Ø±Ú©ÙˆØ±Ø¯ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")
        st.dataframe(pd.DataFrame(ads))
        if st.button("ğŸš€ ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ Gemini"):
            result = llm(ads)
            if result:
                st.json(result)
                st.download_button("ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø®Ø±ÙˆØ¬ÛŒ", data=json.dumps(result, ensure_ascii=False, indent=2),
                                   file_name="company_members.json", mime="application/json")

# --- ØªØ¨ 3 ---
with tab3:
    st.markdown("### ğŸ“ˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ ØªØ­Ù„ÛŒÙ„â€ŒØ´Ø¯Ù‡ Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª (company_members.json)")
    uploaded2 = st.file_uploader("ğŸ“‚ ÙØ§ÛŒÙ„ JSON Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯", type=["json"], key="tab3uploader")
    if uploaded2:
        data = json.load(uploaded2)
        charts(data)
