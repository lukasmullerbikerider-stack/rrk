import os
import time
import json
import logging
import streamlit as st
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime
from jdatetime import datetime as jdatetime
import subprocess
import sys
import tempfile

# ----------------------------------
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ
# ----------------------------------
st.set_page_config(page_title="RRK Company Extractor", page_icon="ğŸ¢", layout="wide")

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Create a temporary directory for each run
user_data_dir = tempfile.mkdtemp()

# Setup Chrome options
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--incognito")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--window-size=1400,1000")
chrome_options.add_argument("--disable-notifications")

# ----------------------------------
# ØªÙˆØ§Ø¨Ø¹ Selenium
# ----------------------------------
def scrape_company_ads(query):
    """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ rrk.ir"""
    driver, wait = setup_driver()
    ad_data = []

    try:
        driver.get("https://www.rrk.ir/")
        search_box = wait.until(EC.presence_of_element_located((By.ID, "P0_SEARCH_ITEM")))
        search_box.clear()
        search_box.send_keys(query)
        driver.find_element(By.ID, "BTN_ADVANCEDSEARCH").click()
        time.sleep(3)

        # ÙˆØ±ÙˆØ¯ Ø¨Ù‡ Ø¨Ø®Ø´ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§
        wait.until(EC.element_to_be_clickable((By.CLASS_NAME, "t-LinksList-link"))).click()
        time.sleep(5)

        current_page = 1
        while True:
            ad_links = get_links(driver)
            if not ad_links:
                break

            for tag in ad_links:
                href = tag.get("href")
                if not href.startswith("/ords/r/rrs/rrs-front/f-detail-ad"):
                    continue
                url = "https://rrk.ir" + href

                driver.execute_script("window.open('');")
                driver.switch_to.window(driver.window_handles[1])
                driver.get(url)
                soup = BeautifulSoup(driver.page_source, "html.parser")

                try:
                    data = extract_fields(driver, soup)
                    data["url"] = url
                    ad_data.append(data)
                except Exception as e:
                    logging.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ú¯Ù‡ÛŒ: {e}")
                finally:
                    driver.close()
                    driver.switch_to.window(driver.window_handles[0])
                    time.sleep(2)

            # ØµÙØ­Ù‡ Ø¨Ø¹Ø¯
            next_buttons = driver.find_elements(By.CSS_SELECTOR, "ul.a-GV-pageSelector-list li button.a-GV-pageButton")
            next_btn = next((b for b in next_buttons if b.text.isdigit() and int(b.text) == current_page + 1), None)
            if not next_btn:
                break
            driver.execute_script("arguments[0].click();", next_btn)
            current_page += 1
            time.sleep(5)

    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø§: {e}")
    finally:
        driver.quit()

    return ad_data

def setup_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--user-data-dir=/tmp/unique_profile_" + str(time.time()))
    # Merge with global chrome_options for better compatibility
    for arg in chrome_options.arguments:
        if arg not in options.arguments:
            options.add_argument(arg.split('=')[0] if '=' in arg else arg)
    driver = webdriver.Chrome(options=options)

    driver.implicitly_wait(10)
    wait = WebDriverWait(driver, 60)
    return driver, wait

def get_links(driver):
    soup = BeautifulSoup(driver.page_source, "html.parser")
    return soup.select("a[href*='/ords/r/rrs/rrs-front/f-detail-ad']")

def extract_fields(driver, soup):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¢Ú¯Ù‡ÛŒ Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª"""
    fields = {
        "Ø´Ù…Ø§Ø±Ù‡ Ù¾ÛŒÚ¯ÛŒØ±ÛŒ": driver.find_element(By.ID, "P28_REFERENCENUMBER").get_attribute("value"),
        "Ø´Ù…Ø§Ø±Ù‡ Ù†Ø§Ù…Ù‡": driver.find_element(By.ID, "P28_INDIKATORNUMBER").get_attribute("value"),
        "ØªØ§Ø±ÛŒØ® Ù†Ø§Ù…Ù‡": driver.find_element(By.ID, "P28_SABTDATE").get_attribute("value"),
        "Ù†Ø§Ù… Ø´Ø±Ú©Øª": driver.find_element(By.ID, "P28_COMPANYNAME").get_attribute("value"),
        "Ø´Ù†Ø§Ø³Ù‡ Ù…Ù„ÛŒ Ø´Ø±Ú©Øª": driver.find_element(By.ID, "P28_SABTNATIONALID").get_attribute("value"),
        "Ø´Ù…Ø§Ø±Ù‡ Ø«Ø¨Øª": driver.find_element(By.ID, "P28_SABTNUMBER").get_attribute("value"),
        "Ø´Ù…Ø§Ø±Ù‡ Ø±ÙˆØ²Ù†Ø§Ù…Ù‡": driver.find_element(By.ID, "P28_NEWSPAPERNO").get_attribute("value"),
        "ØªØ§Ø±ÛŒØ® Ø±ÙˆØ²Ù†Ø§Ù…Ù‡": driver.find_element(By.ID, "P28_NEWSPAPERDATE").get_attribute("value"),
        "Ø´Ù…Ø§Ø±Ù‡ ØµÙØ­Ù‡ Ø±ÙˆØ²Ù†Ø§Ù…Ù‡": driver.find_element(By.ID, "P28_PAGENUMBER").get_attribute("value"),
        "ØªØ¹Ø¯Ø§Ø¯ Ù†ÙˆØ¨Øª Ø§Ù†ØªØ´Ø§Ø±": driver.find_element(By.ID, "P28_HCNEWSSTAGE").get_attribute("value")
    }
    dynamic = soup.select_one("a-dynamic-content")
    fields["Ù…ØªÙ† Ø¢Ú¯Ù‡ÛŒ"] = dynamic.get_text(" ", strip=True) if dynamic else soup.get_text(" ", strip=True)
    return fields

# 2ï¸âƒ£ --- ØªÙ†Ø¸ÛŒÙ… API Key ---
apikey = "AIzaSyA-50zuEBmGJEutQmlQ0lK8X2RqqpQXkf4"
# ------------------ ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ ------------------

def llm(data):
    """Ø§Ø±Ø³Ø§Ù„ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ù…Ø¯Ù„ Gemini Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ø®Ø±ÙˆØ¬ÛŒ JSON"""
    try:
        prompt = json.dumps(data, ensure_ascii=False, indent=2)
        system_instruction = """
        Ù†Ù‚Ø´: ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ø­Ù‚ÙˆÙ‚ÛŒ Ùˆ Ø´Ø±Ú©ØªÛŒ Ù…ØªØ®ØµØµ Ø¯Ø± Ø±ÙˆØ²Ù†Ø§Ù…Ù‡ Ø±Ø³Ù…ÛŒ.
        ÙˆØ¸ÛŒÙÙ‡: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª Ø§Ø² Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ ØªØ§Ø±ÛŒØ®ÛŒ.
        Ø®Ø±ÙˆØ¬ÛŒ JSON Ø·Ø¨Ù‚ Ù‚Ø§Ù„Ø¨ Ù…Ø´Ø®Øµ.
        """

        model = genai.GenerativeModel(
            model_name="gemini-2.5-pro",
            system_instruction=system_instruction
        )

        response = model.generate_content(
            prompt,
            generation_config={
                "response_mime_type": "application/json",
                "temperature": 0.2
            }
        )

        result = json.loads(response.text)
        return result

    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø¯Ù„: {e}")
        return None


def shamsi_to_miladi(date_str):
    if not date_str or date_str == 'null':
        return datetime.now()
    try:
        year, month, day = map(int, date_str.split('/'))
        return jdatetime.date(year, month, day).togregorian()
    except Exception:
        return datetime.now()


def charts(data):
    """Ù†Ù…Ø§ÛŒØ´ Ø³Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ùˆ ØªØ¹Ø§Ù…Ù„ÛŒ Ø§Ø² Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª"""
    if not data:
        st.error("âŒ ÙØ§ÛŒÙ„ JSON Ø®Ø§Ù„ÛŒ Ø§Ø³Øª.")
        return

    # ØªØ±Ú©ÛŒØ¨ Ø§Ø¹Ø¶Ø§ÛŒ ÙØ¹Ù„ÛŒ Ùˆ Ø³Ø§Ø¨Ù‚
    members = []
    if 'Ø§Ø¹Ø¶Ø§ÛŒ ÙØ¹Ù„ÛŒ Ø´Ø±Ú©Øª' in data:
        members.extend(data['Ø§Ø¹Ø¶Ø§ÛŒ ÙØ¹Ù„ÛŒ Ø´Ø±Ú©Øª'])
    if 'Ø§Ø¹Ø¶Ø§ÛŒ Ø³Ø§Ø¨Ù‚ Ø´Ø±Ú©Øª' in data:
        members.extend(data['Ø§Ø¹Ø¶Ø§ÛŒ Ø³Ø§Ø¨Ù‚ Ø´Ø±Ú©Øª'])

    if len(members) == 0:
        st.warning("âš ï¸ Ù‡ÛŒÚ† Ø¹Ø¶ÙˆÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
        return

    df = pd.DataFrame(members)

    # --- ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ® ---
    def shamsi_to_miladi(date_str):
        if not date_str or not isinstance(date_str, str) or '/' not in date_str:
            return datetime.now()
        try:
            y, m, d = map(int, date_str.split('/'))
            return jdatetime.date(y, m, d).togregorian()
        except:
            return datetime.now()

    df["ØªØ§Ø±ÛŒØ®_Ø´Ø±ÙˆØ¹_Ù…ÛŒÙ„Ø§Ø¯ÛŒ"] = df["ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹"].apply(shamsi_to_miladi)
    df["ØªØ§Ø±ÛŒØ®_Ù¾Ø§ÛŒØ§Ù†_Ù…ÛŒÙ„Ø§Ø¯ÛŒ"] = df["ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù†"].apply(shamsi_to_miladi)

    # --- Ø±Ù†Ú¯â€ŒØ¨Ù†Ø¯ÛŒ Ø³Ù…Øªâ€ŒÙ‡Ø§ ---
    color_map = {
        'Ù…Ø¯ÛŒØ±Ø¹Ø§Ù…Ù„': '#FF8C00',
        'Ù…Ø¯ÛŒØ±Ø¹Ø§Ù…Ù„ Ùˆ Ø¹Ø¶Ùˆ Ø§ØµÙ„ÛŒ Ù‡ÛŒØ¦Øª Ù…Ø¯ÛŒØ±Ù‡': '#FF8C00',
        'Ø±Ø¦ÛŒØ³ Ù‡ÛŒØ¦Øª Ù…Ø¯ÛŒØ±Ù‡': '#1f77b4',
        'Ù†Ø§ÛŒØ¨ Ø±Ø¦ÛŒØ³ Ù‡ÛŒØ¦Øª Ù…Ø¯ÛŒØ±Ù‡': '#2ca02c',
        'Ø¹Ø¶Ùˆ Ù‡ÛŒØ¦Øª Ù…Ø¯ÛŒØ±Ù‡': '#9467bd',
        'Ø¹Ø¶Ùˆ Ø§ØµÙ„ÛŒ Ù‡ÛŒØ¦Øª Ù…Ø¯ÛŒØ±Ù‡': '#9467bd',
        'Ø¨Ø§Ø²Ø±Ø³ Ø§ØµÙ„ÛŒ': '#8c564b',
        'Ø¨Ø§Ø²Ø±Ø³ Ø¹Ù„ÛŒ Ø§Ù„Ø¨Ø¯Ù„': '#e377c2'
    }

    # ============================================================
    # 1ï¸âƒ£ Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ§ÛŒÙ…â€ŒÙ„Ø§ÛŒÙ† Ú©Ø§Ù…Ù„ Ø§Ø¹Ø¶Ø§
    # ============================================================
    st.subheader("ğŸ“… ØªØ§ÛŒÙ…â€ŒÙ„Ø§ÛŒÙ† Ú©Ø§Ù…Ù„ Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª")

    fig_all = go.Figure()
    for _, row in df.iterrows():
        color = color_map.get(row["Ø³Ù…Øª"], "#7f7f7f")
        fig_all.add_trace(
            go.Scatter(
                x=[row["ØªØ§Ø±ÛŒØ®_Ø´Ø±ÙˆØ¹_Ù…ÛŒÙ„Ø§Ø¯ÛŒ"], row["ØªØ§Ø±ÛŒØ®_Ù¾Ø§ÛŒØ§Ù†_Ù…ÛŒÙ„Ø§Ø¯ÛŒ"]],
                y=[row["Ù†Ø§Ù…"], row["Ù†Ø§Ù…"]],
                mode="lines+markers",
                name=row["Ø³Ù…Øª"],
                line=dict(color=color, width=5),
                marker=dict(size=8, color=color),
                hovertemplate=f"<b>{row['Ù†Ø§Ù…']}</b><br>{row['Ø³Ù…Øª']}<br>{row['ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹']} ØªØ§ {row['ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù†']}<extra></extra>"
            )
        )

    fig_all.update_layout(
        title="ØªØ§Ø±ÛŒØ®Ú†Ù‡â€ŒÛŒ Ù…Ø³Ø¦ÙˆÙ„ÛŒØª Ø§Ø¹Ø¶Ø§",
        xaxis_title="Ø²Ù…Ø§Ù†",
        yaxis_title="Ù†Ø§Ù… Ø§Ø¹Ø¶Ø§",
        height=700,
        template="plotly_white",
        hovermode="closest"
    )
    st.plotly_chart(fig_all, use_container_width=True)

    # ============================================================
    # 2ï¸âƒ£ Ù†Ù…ÙˆØ¯Ø§Ø± ÙˆÛŒÚ˜Ù‡ Ù…Ø¯ÛŒØ±Ø¹Ø§Ù…Ù„ Ùˆ Ù‡ÛŒØ¦Øª Ù…Ø¯ÛŒØ±Ù‡
    # ============================================================
    st.subheader("ğŸ‘” ØªØ§ÛŒÙ…â€ŒÙ„Ø§ÛŒÙ† Ù…Ø¯ÛŒØ±Ø¹Ø§Ù…Ù„ Ùˆ Ù‡ÛŒØ¦Øªâ€ŒÙ…Ø¯ÛŒØ±Ù‡")

    df_board = df[df["Ø³Ù…Øª"].str.contains("Ù…Ø¯ÛŒØ±Ø¹Ø§Ù…Ù„|Ù‡ÛŒØ¦Øª", na=False)]
    fig_board = go.Figure()
    for _, row in df_board.iterrows():
        color = color_map.get(row["Ø³Ù…Øª"], "#7f7f7f")
        fig_board.add_trace(
            go.Scatter(
                x=[row["ØªØ§Ø±ÛŒØ®_Ø´Ø±ÙˆØ¹_Ù…ÛŒÙ„Ø§Ø¯ÛŒ"], row["ØªØ§Ø±ÛŒØ®_Ù¾Ø§ÛŒØ§Ù†_Ù…ÛŒÙ„Ø§Ø¯ÛŒ"]],
                y=[f"{row['Ù†Ø§Ù…']} ({row['Ø³Ù…Øª']})"] * 2,
                mode="lines+markers",
                line=dict(color=color, width=6),
                marker=dict(size=9, color=color),
                hovertemplate=f"<b>{row['Ù†Ø§Ù…']}</b><br>{row['Ø³Ù…Øª']}<br>{row['ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹']} ØªØ§ {row['ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù†']}<extra></extra>"
            )
        )

    fig_board.update_layout(
        title="ØªØºÛŒÛŒØ±Ø§Øª Ù…Ø¯ÛŒØ±Ø¹Ø§Ù…Ù„ Ùˆ Ø§Ø¹Ø¶Ø§ÛŒ Ù‡ÛŒØ¦Øªâ€ŒÙ…Ø¯ÛŒØ±Ù‡",
        xaxis_title="Ø²Ù…Ø§Ù†",
        yaxis_title="Ø§Ø¹Ø¶Ø§",
        height=600,
        template="plotly_white",
        hovermode="closest"
    )
    st.plotly_chart(fig_board, use_container_width=True)

    # ============================================================
    # 3ï¸âƒ£ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¢Ù…Ø§Ø± Ø³Ù…Øªâ€ŒÙ‡Ø§
    # ============================================================
    st.subheader("ğŸ“Š Ø¢Ù…Ø§Ø± ØªØ¹Ø¯Ø§Ø¯ Ø³Ù…Øªâ€ŒÙ‡Ø§")

    position_counts = df["Ø³Ù…Øª"].value_counts()
    fig_bar = go.Figure(
        go.Bar(
            x=position_counts.index,
            y=position_counts.values,
            text=position_counts.values,
            textposition="auto",
            marker_color="#636EFA"
        )
    )
    fig_bar.update_layout(
        title="ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø± Ù‡Ø± Ø³Ù…Øª Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø´Ø±Ú©Øª",
        xaxis_title="Ø³Ù…Øª",
        yaxis_title="ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙØ¹Ø§Øª",
        height=500,
        template="plotly_white"
    )
    st.plotly_chart(fig_bar, use_container_width=True)


# ------------------ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Streamlit ------------------
st.set_page_config(page_title="RRK Analyzer", layout="wide")
st.title("ğŸ¢ RRK.ir â€“ Company Ads Extractor & Analyzer")

tab1, tab2, tab3 = st.tabs(["ğŸ•µï¸ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø±Ú©Øª", "ğŸ“‚ Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª", "ğŸ“ˆ ØªØ§ÛŒÙ…â€ŒÙ„Ø§ÛŒÙ† Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª"])

# --- ØªØ¨ 1 ---
with tab1:
    st.info("Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø§ ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ù†Ø§Ù… Ø´Ø±Ú©ØªØŒ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ù†ÛŒØ¯ (Ù†Ù…ÙˆÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ).")
    query = st.text_input("ğŸ” Ù†Ø§Ù… Ø´Ø±Ú©Øª ÛŒØ§ Ø´Ù†Ø§Ø³Ù‡ Ù…Ù„ÛŒ:")
    if st.button("Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬"):
        st.warning("Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù‡Ù†ÙˆØ² Ø¨Ù‡ ØªØ§Ø¨Ø¹ scrape_company_ads Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯ (Ø¯Ø± Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡ ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª).")

# --- ØªØ¨ 2: ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ø³Ø§Ø®ØªØ§Ø± Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª ---
with tab2:
    st.markdown("## ğŸ“‚ ØªØ­Ù„ÛŒÙ„ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ø´Ø±Ú©Øª Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Gemini")

    st.info("""
    ğŸ“Œ Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ ÙØ§ÛŒÙ„ JSON Ø­Ø§ÙˆÛŒ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ²Ù†Ø§Ù…Ù‡ Ø±Ø³Ù…ÛŒ (rrk.ir)
    Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ ØªØ§ Ù…Ø¯Ù„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Gemini Ø³Ø§Ø®ØªØ§Ø± Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª (Ù…Ø¯ÛŒØ±Ø¹Ø§Ù…Ù„ØŒ Ù‡ÛŒØ¦Øªâ€ŒÙ…Ø¯ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø²Ø±Ø³â€ŒÙ‡Ø§)
    Ø±Ø§ Ø§Ø² Ø¢Ù† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†Ø¯.
    """)

    uploaded = st.file_uploader("ğŸ“¥ ÙØ§ÛŒÙ„ JSON Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯", type=["json"], key="tab2_file")

    if uploaded is not None:
        try:
            ads = json.load(uploaded)
            if not isinstance(ads, list) or len(ads) == 0:
                st.error("âš ï¸ ÙØ±Ù…Øª ÙØ§ÛŒÙ„ JSON Ø¨Ø§ÛŒØ¯ ÛŒÚ© Ù„ÛŒØ³Øª Ø§Ø² Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§Ø´Ø¯.")
            else:
                st.success(f"âœ… ÙØ§ÛŒÙ„ Ø¨Ø§ {len(ads)} Ø¢Ú¯Ù‡ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")
                st.dataframe(pd.DataFrame(ads))

                # Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ
                st.markdown("### ğŸ§¾ Ø®Ù„Ø§ØµÙ‡ Ø§ÙˆÙ„ÛŒÙ‡ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§")
                companies = {ad.get("Ù†Ø§Ù… Ø´Ø±Ú©Øª") for ad in ads if ad.get("Ù†Ø§Ù… Ø´Ø±Ú©Øª")}
                st.write(f"**ØªØ¹Ø¯Ø§Ø¯ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§:** {len(ads)}")
                st.write(f"**ØªØ¹Ø¯Ø§Ø¯ Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ±Ø¨Ù‡â€ŒÙØ±Ø¯:** {len(companies)}")
                st.write("**Ù†Ø§Ù… Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§:**", "ØŒ ".join(companies))

                # Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„
                st.divider()
                st.markdown("### ğŸ¤– Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„ ØªØ­Ù„ÛŒÙ„ Ø§Ø¹Ø¶Ø§")
                run_ai = st.button("ğŸš€ Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ Gemini", use_container_width=True)

                if run_ai:
                    try:
                        with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ø§Ø±Ø³Ø§Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ù…Ø¯Ù„ Gemini..."):
                            result = llm(ads)

                        if result:
                            st.success("âœ… ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
                            st.json(result)

                            # Ø¯Ú©Ù…Ù‡â€ŒÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø®Ø±ÙˆØ¬ÛŒ
                            st.download_button(
                                "ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø®Ø±ÙˆØ¬ÛŒ JSON ØªØ­Ù„ÛŒÙ„â€ŒØ´Ø¯Ù‡",
                                data=json.dumps(result, ensure_ascii=False, indent=2),
                                file_name="company_members.json",
                                mime="application/json"
                            )
                        else:
                            st.error("âŒ Ù…Ø¯Ù„ Ù‡ÛŒÚ† Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ù†Ú¯Ø±Ø¯Ø§Ù†Ø¯. Ù„Ø·ÙØ§Ù‹ ÙˆØ±ÙˆØ¯ÛŒ JSON Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")

                    except Exception as e:
                        st.error(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„: {e}")
                        st.exception(e)

        except json.JSONDecodeError:
            st.error("âŒ ÙØ§ÛŒÙ„ JSON Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª.")
        except Exception as e:
            st.error(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„: {e}")
            st.exception(e)
    else:
        st.info("Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ØŒ ÙØ§ÛŒÙ„ JSON Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯.")

# --- ØªØ¨ 3: Ù†Ù…Ø§ÛŒØ´ ØªØ§ÛŒÙ…â€ŒÙ„Ø§ÛŒÙ† Ùˆ Ø¢Ù…Ø§Ø± Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª ---
with tab3:
    st.markdown("## ğŸ“ˆ ØªØ§ÛŒÙ…â€ŒÙ„Ø§ÛŒÙ† Ùˆ ØªØ­Ù„ÛŒÙ„ Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª")

    st.info("""
    ğŸ”¹ ÙØ§ÛŒÙ„ JSON Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ (company_members.json) Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ ØªØ§ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯.  
    Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¯Ø§Ø´ØªÙ† ÙØ§ÛŒÙ„ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.
    """)

    col1, col2 = st.columns([3, 1])
    with col1:
        uploaded2 = st.file_uploader("ğŸ“‚ ÙØ§ÛŒÙ„ JSON Ø§Ø¹Ø¶Ø§ÛŒ Ø´Ø±Ú©Øª Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯", type=["json"], key="tab3_file")

    with col2:
        st.markdown("### ÛŒØ§")
        sample_btn = st.button("ğŸ“Š Ù†Ù…Ø§ÛŒØ´ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÛŒ ØªØ³ØªÛŒ")

    if uploaded2 is not None:
        try:
            data = json.load(uploaded2)

            if not isinstance(data, dict):
                st.error("âŒ Ø³Ø§Ø®ØªØ§Ø± ÙØ§ÛŒÙ„ JSON Ø¨Ø§ÛŒØ¯ ÛŒÚ© Ø´ÛŒØ¡ (object) Ø¨Ø§Ø´Ø¯ØŒ Ù†Ù‡ Ù„ÛŒØ³Øª.")
            elif "Ø§Ø¹Ø¶Ø§ÛŒ ÙØ¹Ù„ÛŒ Ø´Ø±Ú©Øª" not in data and "Ø§Ø¹Ø¶Ø§ÛŒ Ø³Ø§Ø¨Ù‚ Ø´Ø±Ú©Øª" not in data:
                st.error("âŒ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ 'Ø§Ø¹Ø¶Ø§ÛŒ ÙØ¹Ù„ÛŒ Ø´Ø±Ú©Øª' ÛŒØ§ 'Ø§Ø¹Ø¶Ø§ÛŒ Ø³Ø§Ø¨Ù‚ Ø´Ø±Ú©Øª' Ø¯Ø± ÙØ§ÛŒÙ„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù†Ø¯.")
                st.json(data)
            else:
                st.success(f"âœ… ÙØ§ÛŒÙ„ '{data.get('Ù†Ø§Ù… Ø´Ø±Ú©Øª', 'â€”')}' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")
                st.write(f"**Ø´Ù†Ø§Ø³Ù‡ Ù…Ù„ÛŒ:** {data.get('Ø´Ù†Ø§Ø³Ù‡ Ø´Ø±Ú©Øª', 'â€”')}")
                st.divider()
                charts(data)
        except json.JSONDecodeError:
            st.error("âŒ ÙØ§ÛŒÙ„ JSON Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª.")
        except Exception as e:
            st.error(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„: {e}")
            st.exception(e)

    elif sample_btn:
        st.warning("âš ï¸ Ø¯Ø± Ø­Ø§Ù„ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡â€ŒÛŒ Ù†Ù…ÙˆÙ†Ù‡...")

        sample_data = {
            "Ù†Ø§Ù… Ø´Ø±Ú©Øª": "Ù†Ù…ÙˆÙ†Ù‡ Ø´Ø±Ú©Øª Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ",
            "Ø´Ù†Ø§Ø³Ù‡ Ø´Ø±Ú©Øª": "123456789",
            "Ø§Ø¹Ø¶Ø§ÛŒ ÙØ¹Ù„ÛŒ Ø´Ø±Ú©Øª": [
                {"Ù†Ø§Ù…": "Ø¹Ù„ÛŒ Ø±Ø¶Ø§ÛŒÛŒ", "Ø³Ù…Øª": "Ù…Ø¯ÛŒØ±Ø¹Ø§Ù…Ù„", "ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹": "1402/01/01", "ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù†": "1404/01/01"},
                {"Ù†Ø§Ù…": "Ø±Ø¶Ø§ Ù…Ø­Ù…Ø¯ÛŒ", "Ø³Ù…Øª": "Ø±Ø¦ÛŒØ³ Ù‡ÛŒØ¦Øª Ù…Ø¯ÛŒØ±Ù‡", "ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹": "1402/01/01", "ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù†": "1404/01/01"},
                {"Ù†Ø§Ù…": "Ù…Ø±ÛŒÙ… Ø³Ù‡Ø±Ø§Ø¨ÛŒ", "Ø³Ù…Øª": "Ø¨Ø§Ø²Ø±Ø³ Ø§ØµÙ„ÛŒ", "ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹": "1402/01/01", "ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù†": "ØªØ§ Ù¾Ø§ÛŒØ§Ù† Ø³Ø§Ù„ Ù…Ø§Ù„ÛŒ"}
            ],
            "Ø§Ø¹Ø¶Ø§ÛŒ Ø³Ø§Ø¨Ù‚ Ø´Ø±Ú©Øª": [
                {"Ù†Ø§Ù…": "Ø­Ø³ÛŒÙ† Ú©Ø±Ù…ÛŒ", "Ø³Ù…Øª": "Ù…Ø¯ÛŒØ±Ø¹Ø§Ù…Ù„", "ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹": "1398/01/01", "ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù†": "1400/01/01"},
                {"Ù†Ø§Ù…": "Ù†Ø§Ù‡ÛŒØ¯ Ø§Ø­Ù…Ø¯ÛŒ", "Ø³Ù…Øª": "Ø±Ø¦ÛŒØ³ Ù‡ÛŒØ¦Øª Ù…Ø¯ÛŒØ±Ù‡", "ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹": "1398/01/01", "ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù†": "1400/01/01"}
            ]
        }

        st.divider()
        charts(sample_data)

    else:
        st.info("Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ØŒ ÙØ§ÛŒÙ„ JSON Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÛŒ ØªØ³ØªÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.")
